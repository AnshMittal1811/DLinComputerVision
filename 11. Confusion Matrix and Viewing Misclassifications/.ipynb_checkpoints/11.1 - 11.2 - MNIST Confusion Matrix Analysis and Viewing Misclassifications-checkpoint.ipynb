{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Number of Classes: 10\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 189s 3ms/step - loss: 0.2667 - acc: 0.9187 - val_loss: 0.0556 - val_acc: 0.9819\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 170s 3ms/step - loss: 0.0889 - acc: 0.9740 - val_loss: 0.0396 - val_acc: 0.9864\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 192s 3ms/step - loss: 0.0670 - acc: 0.9805 - val_loss: 0.0350 - val_acc: 0.9884\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 194s 3ms/step - loss: 0.0553 - acc: 0.9838 - val_loss: 0.0321 - val_acc: 0.9897\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 204s 3ms/step - loss: 0.0464 - acc: 0.9856 - val_loss: 0.0282 - val_acc: 0.9907\n",
      "Test loss: 0.028205487172584982\n",
      "Test accuracy: 0.9907\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# Training Parameters\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "# loads the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test)  = mnist.load_data()\n",
    "\n",
    "# Lets store the number of rows and columns\n",
    "img_rows = x_train[0].shape[0]\n",
    "img_cols = x_train[1].shape[0]\n",
    "\n",
    "# Getting our date in the right 'shape' needed for Keras\n",
    "# We need to add a 4th dimenion to our date thereby changing our\n",
    "# Our original image shape of (60000,28,28) to (60000,28,28,1)\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "# store the shape of a single image \n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# change our image type to float32 data type\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize our data by changing the range from (0 to 255) to (0 to 1)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Now we one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# Let's count the number columns in our hot encoded matrix \n",
    "print (\"Number of Classes: \" + str(y_test.shape[1]))\n",
    "\n",
    "num_classes = y_test.shape[1]\n",
    "num_pixels = x_train.shape[1] * x_train.shape[2]\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = keras.optimizers.Adadelta(),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       980\n",
      "           1       0.99      1.00      1.00      1135\n",
      "           2       0.98      1.00      0.99      1032\n",
      "           3       0.99      0.99      0.99      1010\n",
      "           4       0.99      0.99      0.99       982\n",
      "           5       0.99      0.99      0.99       892\n",
      "           6       0.99      0.99      0.99       958\n",
      "           7       0.99      0.98      0.99      1028\n",
      "           8       0.99      0.99      0.99       974\n",
      "           9       0.99      0.98      0.99      1009\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     10000\n",
      "   macro avg       0.99      0.99      0.99     10000\n",
      "weighted avg       0.99      0.99      0.99     10000\n",
      "\n",
      "[[ 977    0    0    0    0    0    1    1    1    0]\n",
      " [   0 1130    3    1    0    0    1    0    0    0]\n",
      " [   1    0 1029    0    1    0    0    1    0    0]\n",
      " [   0    0    4 1003    0    1    0    1    1    0]\n",
      " [   0    0    0    0  976    0    0    0    2    4]\n",
      " [   3    0    0    5    0  881    3    0    0    0]\n",
      " [   6    2    0    0    1    1  948    0    0    0]\n",
      " [   1    2   11    2    0    0    0 1010    1    1]\n",
      " [   4    0    3    0    0    0    0    2  963    2]\n",
      " [   1    2    0    1    5    3    0    2    5  990]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_pred = model.predict_classes(x_test)\n",
    "\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred))\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of misclassifed data are: \n",
      "\n",
      "(array([ 259,  320,  445,  582,  684,  740,  947, 1014, 1039, 1226, 1232,\n",
      "       1247, 1260, 1326, 1364, 1393, 1414, 1530, 1621, 1709, 1717, 1754,\n",
      "       1901, 2035, 2040, 2043, 2109, 2118, 2129, 2130, 2135, 2182, 2189,\n",
      "       2293, 2369, 2387, 2406, 2414, 2462, 2488, 2578, 2597, 2654, 2760,\n",
      "       2896, 2921, 2927, 2939, 3030, 3060, 3073, 3422, 3503, 3520, 3558,\n",
      "       3597, 3751, 3767, 3778, 3808, 3906, 4007, 4075, 4176, 4238, 4256,\n",
      "       4443, 4497, 4507, 4639, 4740, 4761, 4807, 4814, 4860, 5246, 5887,\n",
      "       5937, 5955, 6576, 6597, 6625, 6651, 6783, 7434, 8527, 9009, 9015,\n",
      "       9019, 9530, 9729, 9770, 9982]),)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test)  = mnist.load_data()\n",
    "\n",
    "result = np.absolute(y_test - y_pred)\n",
    "print(\"Indices of misclassifed data are: \\n\\n\" + str(np.nonzero(result > 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [ 259,  320,  445,  582,  684,  740,  947, 1014, 1039, 1226, 1232,\n",
    "       1247, 1260, 1326, 1364, 1393, 1414, 1530, 1621, 1709, 1717, 1754,\n",
    "       1901, 2035, 2040, 2043, 2109, 2118, 2129, 2130, 2135, 2182, 2189,\n",
    "       2293, 2369, 2387, 2406, 2414, 2462, 2488, 2578, 2597, 2654, 2760,\n",
    "       2896, 2921, 2927, 2939, 3030, 3060, 3073, 3422, 3503, 3520, 3558,\n",
    "       3597, 3751, 3767, 3778, 3808, 3906, 4007, 4075, 4176, 4238, 4256,\n",
    "       4443, 4497, 4507, 4639, 4740, 4761, 4807, 4814, 4860, 5246, 5887,\n",
    "       5937, 5955, 6576, 6597, 6625, 6651, 6783, 7434, 8527, 9009, 9015,\n",
    "       9019, 9530, 9729, 9770, 9982]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_test(name, pred, input_im):\n",
    "    BLACK = [0,0,0]\n",
    "    expanded_image = cv2.copyMakeBorder(input_im, 0, 0, 0, imageL.shape[0] ,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    expanded_image = cv2.cvtColor(expanded_image, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.putText(expanded_image, str(pred), (152, 70) , cv2.FONT_HERSHEY_COMPLEX_SMALL,4, (0,255,0), 2)\n",
    "    cv2.imshow(name, expanded_image)\n",
    "\n",
    "for i in range(0,10):\n",
    "\n",
    "    input_im = x_test[result[i]]\n",
    "\n",
    "    imageL = cv2.resize(input_im, None, fx=4, fy=4, interpolation = cv2.INTER_CUBIC) \n",
    "    input_im = input_im.reshape(1,28,28,1) \n",
    "    \n",
    "    ## Get Prediction\n",
    "    res = str(classifier.predict_classes(input_im, 1, verbose = 0)[0])\n",
    "    draw_test(\"Prediction\", res, imageL) \n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
